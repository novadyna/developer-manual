{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Nova Developer Manual This is a developer's manual for novaweb.live. This manual assumes you are already familiar with the business domain. That is, the type of service Nova provides, and the format in which it is provided. This site is a work in progress, and should be updated continuously. This document's git repository . This manual is built with MkDocs , please refer to the documentation on how to update an MkDocs site. Serve this site locally Make sure you have python installed and python and pip is in your path. Install MkDocs with pip install mkdocs . Run mkdocs serve . Build Run mkdocs build . Updating The Manual There is no need to build and upload since this manual is hosted with Gitlab Pages. CICD pipeline has been set up, every new commit on master will trigger a re-build. To update the manual, simply commit, and push. Refer to .gitlab-ci.yml under root to see what is being run.","title":"Home"},{"location":"index.html#nova-developer-manual","text":"This is a developer's manual for novaweb.live. This manual assumes you are already familiar with the business domain. That is, the type of service Nova provides, and the format in which it is provided. This site is a work in progress, and should be updated continuously. This document's git repository . This manual is built with MkDocs , please refer to the documentation on how to update an MkDocs site.","title":"Nova Developer Manual"},{"location":"index.html#serve-this-site-locally","text":"Make sure you have python installed and python and pip is in your path. Install MkDocs with pip install mkdocs . Run mkdocs serve .","title":"Serve this site locally"},{"location":"index.html#build","text":"Run mkdocs build .","title":"Build"},{"location":"index.html#updating-the-manual","text":"There is no need to build and upload since this manual is hosted with Gitlab Pages. CICD pipeline has been set up, every new commit on master will trigger a re-build. To update the manual, simply commit, and push. Refer to .gitlab-ci.yml under root to see what is being run.","title":"Updating The Manual"},{"location":"backend/codebase-intro.html","text":"Introduction to the Nova Codebase(Backend) Rough Outline The codebase can generally be split into 4 levels of abstraction. They are: Business Easy to read Business logic only Calls Facade/Storage level modules Facade Easy to read with some business domain knowledge Consolidates multiple Storage level operations + reusable logic Calls Storage level modules Storage Harder to read without a deeper understanding of business logic Your main gateway of interaction with DynamoDB, and in some cases, Redis Calls integration level modules Integration Very hard to read without AWS resource specific knowledge Lower level Calls third party sdks, mainly integrates with AWS resources. Business Level Example of a business level function: export async function handler(event, context) { const {email, password} = JSON.parse(event.body); if(!email || !password) { return failure(ErrorResponse.MISSING_CREDENTIAL); } const sanitizedEmail = email.toLowerCase().trim(); const cred = await adminStorage.tryGetCredential(sanitizedEmail); if(!cred) { return failure(ErrorResponse.INVALID_CREDENTIAL); } const {userId} = cred; const fiveDays = 1000 * 60 * 60 * 24 * 5; if(await adminStorage.matchPassword(userId, password)) { const session = await StorageFacade.invalidateAndCreateSession(userId, fiveDays); const profile = await adminStorage.getProfile(userId); return success({ profile, session }); } else { return failure(ErrorResponse.INVALID_CREDENTIAL); } } The name of the function has been deliberately removed, but can you guess what this function does? Most people can probably guess with a pretty high accuracy the purpose of this function, even non programmers. It is because this is solely declarative, written in plain English, with little to no black magic. Facade Level A simplified excerpt of the StorageFacade that was introduced earlier. export default class StorageFacade { static async createAdminUser(email, password, firstName, lastName, role, credType) { const sanitizedEmail = email.toLowerCase().trim(); const profile = AdminFactory.getNewProfile(sanitizedEmail, firstName, lastName, role); const {userId} = profile; return await Promise.all([ adminStorage.putProfile(userId, profile), adminStorage.putCredential(userId, email, credType), adminStorage.setPassword(userId, password) ]); } } Still relatively readable. Storage Level A simplified excerpt of AdminStorage that both the login function and StorageFacade uses. class AdminStorage extends CompositeStorage { //.... omitted async putProfile(userId, record) { return await this.putSingleton(userId, this.#profileKey, record).then(cleanUpDbFields); } async getProfile(userId) { return this.getSingleton(userId, this.#profileKey).then(cleanUpDbFields); } async putCredential(userId, identifier, credType) { if(await this.credentialExists(identifier)) { throw \"Trying to put a duplicate credential record!\"; } return super.putToCollection(userId, { userId, identifier, credType }, this.#credentialKey, identifier).then(cleanUpDbFields); } async tryGetCredential(identifier) { const creds = await DynamoDB.queryIndex(this.table, reverseIndex, {[this.sortName]: this.genKey(this.#credentialKey, identifier)}); if(creds.length > 0) { return cleanUpDbFields(creds[0]); } else { return null; } } //.... omitted } You may have noticed immediately that, readability dropped by quite a bit. Understanding a Storage class requires knowing how DynamoDB stores data, what our concept of a singleton and a collection is, and what Local and Global Secondary Indexes are and how they work. It is not recommended for newcomers to immediately start creating or modifying a Storage class. A separate entry will be written on our data structure and DynamoDB fundamentals. Integration These classes deal with integration with AWS/other 3rd party services. Excerpt from the DDBClient class. export class DDBClient { // ...much omitted async get(key: any, projection?: Projections, consistent?: boolean): Promise<any> { const param: any = { TableName: this.table, Key: key, ConsistentRead: consistent }; const builder = new ExpressionBuilder(); if(projection) { projectionPaths2Expression(projection, builder); } Object.assign(param, builder.build()); return docClient.get(param).promise().then(res => res.Item); } async update(key: any, action: UpdateAction, conditions?: Conditions): Promise<any> { const param: any = { TableName: this.table, Key: key, ReturnValues: \"ALL_NEW\" }; const builder = new ExpressionBuilder(); action.addToBuilder(builder); if(conditions) { conditions.addToBuilder(builder) } Object.assign(param, builder.build()); return docClient.update(param).promise().then(res => res.Attributes).catch(e => { if(e.code === \"ConditionalCheckFailedException\") { return null; } throw e; }); } async batchWrite(items: any[], recursive: boolean = true): Promise<BatchWriteResult> { return this.batch(items, [], recursive); } async batchDelete(keys: any[], recursive: boolean = true) { return this.batch([], keys, recursive); } async batch(items: any[], delKeys: any[], recursive: boolean): Promise<BatchResult> { const result: BatchResult = { unprocessedWrites: [], unprocessedDeletes: [] }; const requests: any[] = [...items.map(i => ({ PutRequest: {Item: i} })), ...delKeys.map(k => ({ DeleteRequest: {Key: k} }))]; do { const removed = requests.splice(0, 25); const output = await docClient.batchWrite({ RequestItems: { [this.table]: removed } }).promise(); if(output.UnprocessedItems && output.UnprocessedItems[this.table]) { for(const item of output.UnprocessedItems[this.table]) { if(item.PutRequest) { result.unprocessedWrites.push(item.PutRequest.Item); } else if(item.DeleteRequest) { result.unprocessedDeletes.push(item.DeleteRequest.Key); } } } } while(recursive && requests.length > 0); if(requests.length > 0) { for(const request of requests) { if(request.PutRequest) { result.unprocessedWrites.push(request.PutRequest.Item); } else if(request.DeleteRequest) { result.unprocessedDeletes.push(request.DeleteRequest.Key); } } } return result; } } Needless to say, readability is very low without intimate knowledge of how AWS-SDK works and the syntax of a DynamoDB operation. These topics are out of the scope of this manual. Official references: AWS JS SDK DynamoDB Expressions","title":"Introduction to the Nova Codebase(Backend)"},{"location":"backend/codebase-intro.html#introduction-to-the-nova-codebasebackend","text":"","title":"Introduction to the Nova Codebase(Backend)"},{"location":"backend/codebase-intro.html#rough-outline","text":"The codebase can generally be split into 4 levels of abstraction. They are: Business Easy to read Business logic only Calls Facade/Storage level modules Facade Easy to read with some business domain knowledge Consolidates multiple Storage level operations + reusable logic Calls Storage level modules Storage Harder to read without a deeper understanding of business logic Your main gateway of interaction with DynamoDB, and in some cases, Redis Calls integration level modules Integration Very hard to read without AWS resource specific knowledge Lower level Calls third party sdks, mainly integrates with AWS resources.","title":"Rough Outline"},{"location":"backend/codebase-intro.html#business-level","text":"Example of a business level function: export async function handler(event, context) { const {email, password} = JSON.parse(event.body); if(!email || !password) { return failure(ErrorResponse.MISSING_CREDENTIAL); } const sanitizedEmail = email.toLowerCase().trim(); const cred = await adminStorage.tryGetCredential(sanitizedEmail); if(!cred) { return failure(ErrorResponse.INVALID_CREDENTIAL); } const {userId} = cred; const fiveDays = 1000 * 60 * 60 * 24 * 5; if(await adminStorage.matchPassword(userId, password)) { const session = await StorageFacade.invalidateAndCreateSession(userId, fiveDays); const profile = await adminStorage.getProfile(userId); return success({ profile, session }); } else { return failure(ErrorResponse.INVALID_CREDENTIAL); } } The name of the function has been deliberately removed, but can you guess what this function does? Most people can probably guess with a pretty high accuracy the purpose of this function, even non programmers. It is because this is solely declarative, written in plain English, with little to no black magic.","title":"Business Level"},{"location":"backend/codebase-intro.html#facade-level","text":"A simplified excerpt of the StorageFacade that was introduced earlier. export default class StorageFacade { static async createAdminUser(email, password, firstName, lastName, role, credType) { const sanitizedEmail = email.toLowerCase().trim(); const profile = AdminFactory.getNewProfile(sanitizedEmail, firstName, lastName, role); const {userId} = profile; return await Promise.all([ adminStorage.putProfile(userId, profile), adminStorage.putCredential(userId, email, credType), adminStorage.setPassword(userId, password) ]); } } Still relatively readable.","title":"Facade Level"},{"location":"backend/codebase-intro.html#storage-level","text":"A simplified excerpt of AdminStorage that both the login function and StorageFacade uses. class AdminStorage extends CompositeStorage { //.... omitted async putProfile(userId, record) { return await this.putSingleton(userId, this.#profileKey, record).then(cleanUpDbFields); } async getProfile(userId) { return this.getSingleton(userId, this.#profileKey).then(cleanUpDbFields); } async putCredential(userId, identifier, credType) { if(await this.credentialExists(identifier)) { throw \"Trying to put a duplicate credential record!\"; } return super.putToCollection(userId, { userId, identifier, credType }, this.#credentialKey, identifier).then(cleanUpDbFields); } async tryGetCredential(identifier) { const creds = await DynamoDB.queryIndex(this.table, reverseIndex, {[this.sortName]: this.genKey(this.#credentialKey, identifier)}); if(creds.length > 0) { return cleanUpDbFields(creds[0]); } else { return null; } } //.... omitted } You may have noticed immediately that, readability dropped by quite a bit. Understanding a Storage class requires knowing how DynamoDB stores data, what our concept of a singleton and a collection is, and what Local and Global Secondary Indexes are and how they work. It is not recommended for newcomers to immediately start creating or modifying a Storage class. A separate entry will be written on our data structure and DynamoDB fundamentals.","title":"Storage Level"},{"location":"backend/codebase-intro.html#integration","text":"These classes deal with integration with AWS/other 3rd party services. Excerpt from the DDBClient class. export class DDBClient { // ...much omitted async get(key: any, projection?: Projections, consistent?: boolean): Promise<any> { const param: any = { TableName: this.table, Key: key, ConsistentRead: consistent }; const builder = new ExpressionBuilder(); if(projection) { projectionPaths2Expression(projection, builder); } Object.assign(param, builder.build()); return docClient.get(param).promise().then(res => res.Item); } async update(key: any, action: UpdateAction, conditions?: Conditions): Promise<any> { const param: any = { TableName: this.table, Key: key, ReturnValues: \"ALL_NEW\" }; const builder = new ExpressionBuilder(); action.addToBuilder(builder); if(conditions) { conditions.addToBuilder(builder) } Object.assign(param, builder.build()); return docClient.update(param).promise().then(res => res.Attributes).catch(e => { if(e.code === \"ConditionalCheckFailedException\") { return null; } throw e; }); } async batchWrite(items: any[], recursive: boolean = true): Promise<BatchWriteResult> { return this.batch(items, [], recursive); } async batchDelete(keys: any[], recursive: boolean = true) { return this.batch([], keys, recursive); } async batch(items: any[], delKeys: any[], recursive: boolean): Promise<BatchResult> { const result: BatchResult = { unprocessedWrites: [], unprocessedDeletes: [] }; const requests: any[] = [...items.map(i => ({ PutRequest: {Item: i} })), ...delKeys.map(k => ({ DeleteRequest: {Key: k} }))]; do { const removed = requests.splice(0, 25); const output = await docClient.batchWrite({ RequestItems: { [this.table]: removed } }).promise(); if(output.UnprocessedItems && output.UnprocessedItems[this.table]) { for(const item of output.UnprocessedItems[this.table]) { if(item.PutRequest) { result.unprocessedWrites.push(item.PutRequest.Item); } else if(item.DeleteRequest) { result.unprocessedDeletes.push(item.DeleteRequest.Key); } } } } while(recursive && requests.length > 0); if(requests.length > 0) { for(const request of requests) { if(request.PutRequest) { result.unprocessedWrites.push(request.PutRequest.Item); } else if(request.DeleteRequest) { result.unprocessedDeletes.push(request.DeleteRequest.Key); } } } return result; } } Needless to say, readability is very low without intimate knowledge of how AWS-SDK works and the syntax of a DynamoDB operation. These topics are out of the scope of this manual. Official references: AWS JS SDK DynamoDB Expressions","title":"Integration"},{"location":"backend/serverless-intro.html","text":"Introduction to the Serverless Framework Nova's backend utilizes the Serverless Framework . It's a toolkit for declaring and deploying cloud resources. Most people use it more for managing a RESTFUL API(this is our main use case), less so for actual resource creation(although you very well can). Creating API Gateway resources manually(even with CloudFormation and/or AWSCDK )\\ is a cumbersome process involving many interdependent resource declarations. Serverless simplifies that process by a lot. The core of a serverless project is the serverless.yml . It is the main declaration file in which you'd declare your endpoints(or other resources). This is a simplified version of the serverless.yml in use for nwv2-api-admin-iam : service: ${file(./.env.${self:provider.stage}.json):projectName}-api-admin-iam provider: name: aws runtime: nodejs12.x stage: ${opt:stage, 'dev'} region: ${opt:region, 'ap-southeast-1'} profile: ${opt:profile, 'nwv2Admin'} environment: AdminTable: ${file(./.env.${self:provider.stage}.json):adminTable} EmailFrom: ${file(./.env.${self:provider.stage}.json):emailFrom} apiGateway: restApiId: ${file(./.env.${self:provider.stage}.json):restApiId} restApiRootResourceId: ${file(./.env.${self:provider.stage}.json):restApiRootResourceId} iamRoleStatements: - Effect: Allow Action: - dynamodb:* Resource: arn:aws:dynamodb:${self:provider.region}:*:* functions: sharedAdminAuthorizer: handler: functions/admin-iam-authorizer.handler createUser: handler: functions/create-user.handler events: - http: path: admin-iam/users method: post authorizer: type: CUSTOM authorizerId: Ref: CustomAdminAuthorizer cors: true authenticate: handler: functions/authenticate.handler events: - http: path: admin-iam/authenticate method: post cors: true Let's break it down bit by bit: service: ${file(./.env.${self:provider.stage}.json):projectName}-api-admin-iam Name of the project, the actual CloudFormation stack deployed will be built off of this, if you modify the service name and redeploy, it will be deployed to a new stack instead of updating the old one. provider: environment: AdminTable: ${file(./.env.${self:provider.stage}.json):adminTable} EmailFrom: ${file(./.env.${self:provider.stage}.json):emailFrom} This is where environment variables are declared. Anything under provider.environment will be placed into process.env for runtime access. What you see there is Serverless 's variable resolution syntax. Reference . Assuming stage = dev , ${file(./.env.${self:provider.stage}.json):emailFrom} will be resolved to the value of the emailFrom attribute inside .env.dev.json under project root. apiGateway: restApiId: ${file(./.env.${self:provider.stage}.json):restApiId} restApiRootResourceId: ${file(./.env.${self:provider.stage}.json):restApiRootResourceId} This property is usually not required, since Serverless creates a new API Gateway from scratch if one is not provided. But since we create our API Gateway resource with the AWSCDK , we provide the IDs here to let Serverless know that we want to build our endpoints under an existing API Gateway rather than a new one. functions: authenticate: aws handler: functions/authenticate.handler events: - http: path: admin-iam/authenticate method: post cors: true Attributes under functions will be treated as a Lambda function and deployed as such. handler points it towards a file that contains the function code. events are triggers that can invoke this function, http means this is triggered by the invocation of an endpoint. For a full reference to all available attributes in serverless.yml , please refer to the official serverless.yml reference .","title":"Introduction to the Serverless Framework"},{"location":"backend/serverless-intro.html#introduction-to-the-serverless-framework","text":"Nova's backend utilizes the Serverless Framework . It's a toolkit for declaring and deploying cloud resources. Most people use it more for managing a RESTFUL API(this is our main use case), less so for actual resource creation(although you very well can). Creating API Gateway resources manually(even with CloudFormation and/or AWSCDK )\\ is a cumbersome process involving many interdependent resource declarations. Serverless simplifies that process by a lot. The core of a serverless project is the serverless.yml . It is the main declaration file in which you'd declare your endpoints(or other resources). This is a simplified version of the serverless.yml in use for nwv2-api-admin-iam : service: ${file(./.env.${self:provider.stage}.json):projectName}-api-admin-iam provider: name: aws runtime: nodejs12.x stage: ${opt:stage, 'dev'} region: ${opt:region, 'ap-southeast-1'} profile: ${opt:profile, 'nwv2Admin'} environment: AdminTable: ${file(./.env.${self:provider.stage}.json):adminTable} EmailFrom: ${file(./.env.${self:provider.stage}.json):emailFrom} apiGateway: restApiId: ${file(./.env.${self:provider.stage}.json):restApiId} restApiRootResourceId: ${file(./.env.${self:provider.stage}.json):restApiRootResourceId} iamRoleStatements: - Effect: Allow Action: - dynamodb:* Resource: arn:aws:dynamodb:${self:provider.region}:*:* functions: sharedAdminAuthorizer: handler: functions/admin-iam-authorizer.handler createUser: handler: functions/create-user.handler events: - http: path: admin-iam/users method: post authorizer: type: CUSTOM authorizerId: Ref: CustomAdminAuthorizer cors: true authenticate: handler: functions/authenticate.handler events: - http: path: admin-iam/authenticate method: post cors: true Let's break it down bit by bit: service: ${file(./.env.${self:provider.stage}.json):projectName}-api-admin-iam Name of the project, the actual CloudFormation stack deployed will be built off of this, if you modify the service name and redeploy, it will be deployed to a new stack instead of updating the old one. provider: environment: AdminTable: ${file(./.env.${self:provider.stage}.json):adminTable} EmailFrom: ${file(./.env.${self:provider.stage}.json):emailFrom} This is where environment variables are declared. Anything under provider.environment will be placed into process.env for runtime access. What you see there is Serverless 's variable resolution syntax. Reference . Assuming stage = dev , ${file(./.env.${self:provider.stage}.json):emailFrom} will be resolved to the value of the emailFrom attribute inside .env.dev.json under project root. apiGateway: restApiId: ${file(./.env.${self:provider.stage}.json):restApiId} restApiRootResourceId: ${file(./.env.${self:provider.stage}.json):restApiRootResourceId} This property is usually not required, since Serverless creates a new API Gateway from scratch if one is not provided. But since we create our API Gateway resource with the AWSCDK , we provide the IDs here to let Serverless know that we want to build our endpoints under an existing API Gateway rather than a new one. functions: authenticate: aws handler: functions/authenticate.handler events: - http: path: admin-iam/authenticate method: post cors: true Attributes under functions will be treated as a Lambda function and deployed as such. handler points it towards a file that contains the function code. events are triggers that can invoke this function, http means this is triggered by the invocation of an endpoint. For a full reference to all available attributes in serverless.yml , please refer to the official serverless.yml reference .","title":"Introduction to the Serverless Framework"},{"location":"backend/analytics/aggregation.html","text":"Aggregation Apart from client generated heartbeats every 30 seconds, we also aggregate data on a 5 minutes interval. Relevant excerpt from nwv2-api-analytics 's serverless.yml : Feel free to look inside the function to gain a sense of what it does. In short, it does the following things, one after the other: findAndAggregateLiveEvents This is the function that gets run every 5 minutes, it finds all events with live viewers(defined as be any events with activities within the last 5 minutes), and sends them down an SQS queue, one which is subscribed by a lambda function. sqsAggregateEventData This is the consumer for the said queue in the last section. It aggregates the data collected, and enables the following metrics: Max Concurrent Viewers Count the number of active users within the past 5 minutes, and tries to update DB record if larger. Total Attended Viewers Much like the last one, get a count of all viewers who has logged in at least once, and tries to update DB record, if larger. Event Stats Please refer to heartbeats chapter for a description of this. Gets a list of stats records within the past 5 minutes, aggregates them, and writes to DB. After aggregating data for events, it moves on the find sessions with live viewers in the past 5 minutes, and sends them down the line to be processed by another function. sqsAggregateSessionData Does pretry much the exact same thing as sqsAggregateEventData except scoped to sessions.","title":"Aggregation"},{"location":"backend/analytics/aggregation.html#aggregation","text":"Apart from client generated heartbeats every 30 seconds, we also aggregate data on a 5 minutes interval. Relevant excerpt from nwv2-api-analytics 's serverless.yml : Feel free to look inside the function to gain a sense of what it does. In short, it does the following things, one after the other:","title":"Aggregation"},{"location":"backend/analytics/aggregation.html#findandaggregateliveevents","text":"This is the function that gets run every 5 minutes, it finds all events with live viewers(defined as be any events with activities within the last 5 minutes), and sends them down an SQS queue, one which is subscribed by a lambda function.","title":"findAndAggregateLiveEvents"},{"location":"backend/analytics/aggregation.html#sqsaggregateeventdata","text":"This is the consumer for the said queue in the last section. It aggregates the data collected, and enables the following metrics:","title":"sqsAggregateEventData"},{"location":"backend/analytics/aggregation.html#max-concurrent-viewers","text":"Count the number of active users within the past 5 minutes, and tries to update DB record if larger.","title":"Max Concurrent Viewers"},{"location":"backend/analytics/aggregation.html#total-attended-viewers","text":"Much like the last one, get a count of all viewers who has logged in at least once, and tries to update DB record, if larger.","title":"Total Attended Viewers"},{"location":"backend/analytics/aggregation.html#event-stats","text":"Please refer to heartbeats chapter for a description of this. Gets a list of stats records within the past 5 minutes, aggregates them, and writes to DB. After aggregating data for events, it moves on the find sessions with live viewers in the past 5 minutes, and sends them down the line to be processed by another function.","title":"Event Stats"},{"location":"backend/analytics/aggregation.html#sqsaggregatesessiondata","text":"Does pretry much the exact same thing as sqsAggregateEventData except scoped to sessions.","title":"sqsAggregateSessionData"},{"location":"backend/analytics/heartbeats.html","text":"Heartbeats This is the hottest path on the Nova platform, every viewer triggers this endpoint every 30 seconds, on every page. This means that if there are 4 thousand viewers currently using the platform, heartbeat will be triggered 8000 times a minute. What does it do? Loads a user's profile from DB, immediately cache it for faster access next time. And then perform operations that enable the following metrics. The Redis data sets that supports the following are detailed in the next chapter, these 2 chapters should be read in parallel. Viewing Sessions A viewing session represent a continuous period of time a user spends on the platform. A user can have multiple viewing sessions. Try to get a user's latest Viewing Session End Time If there isn't one, or it has a timestamp that is considered expired(older than a threshold): Set user's latest start time to Date.now() Set their latest end time to now as well Create a DB record If one exists and isn't too old: Get a user's viewing session start time Set their latest end time to now Update a DB record Alive viewers count (Current viewing users) We consider viewers that submitted a heartbeat within the past 65 seconds \"active\". Logs a heartbeat(with user id and timestamp only), to a redis sorted set. Logs another heartbeat to another set scoped to sessions if the url path indicates the viewer is watching a session. Attendance At the same time we also logs the viewer's ID to a redis set. We can then obtain a count of viewers who has at least visited the platform once by issuing a cardinality command. Event Statistics \"Stats\" is what we call an aggregated metric that involves viewer count grouped by user group and country, this is admittedly not the best name for it. They look something like this: { \"viewerCount\": 100, \"timestamp\": 1637661623070, \"countByUserGroup\": { \"047417b2-4930-4ff0-a3a0-8572cc559005\": 100 }, \"countByViewingCountry\": { \"HK\": 50, \"US\": 50 } } Each record represents a 5 minutes time frame(this will become clear in later chapters concerning aggregation).","title":"Heartbeats"},{"location":"backend/analytics/heartbeats.html#heartbeats","text":"This is the hottest path on the Nova platform, every viewer triggers this endpoint every 30 seconds, on every page. This means that if there are 4 thousand viewers currently using the platform, heartbeat will be triggered 8000 times a minute.","title":"Heartbeats"},{"location":"backend/analytics/heartbeats.html#what-does-it-do","text":"Loads a user's profile from DB, immediately cache it for faster access next time. And then perform operations that enable the following metrics. The Redis data sets that supports the following are detailed in the next chapter, these 2 chapters should be read in parallel.","title":"What does it do?"},{"location":"backend/analytics/heartbeats.html#viewing-sessions","text":"A viewing session represent a continuous period of time a user spends on the platform. A user can have multiple viewing sessions. Try to get a user's latest Viewing Session End Time If there isn't one, or it has a timestamp that is considered expired(older than a threshold): Set user's latest start time to Date.now() Set their latest end time to now as well Create a DB record If one exists and isn't too old: Get a user's viewing session start time Set their latest end time to now Update a DB record","title":"Viewing Sessions"},{"location":"backend/analytics/heartbeats.html#alive-viewers-count-current-viewing-users","text":"We consider viewers that submitted a heartbeat within the past 65 seconds \"active\". Logs a heartbeat(with user id and timestamp only), to a redis sorted set. Logs another heartbeat to another set scoped to sessions if the url path indicates the viewer is watching a session.","title":"Alive viewers count (Current viewing users)"},{"location":"backend/analytics/heartbeats.html#attendance","text":"At the same time we also logs the viewer's ID to a redis set. We can then obtain a count of viewers who has at least visited the platform once by issuing a cardinality command.","title":"Attendance"},{"location":"backend/analytics/heartbeats.html#event-statistics","text":"\"Stats\" is what we call an aggregated metric that involves viewer count grouped by user group and country, this is admittedly not the best name for it. They look something like this: { \"viewerCount\": 100, \"timestamp\": 1637661623070, \"countByUserGroup\": { \"047417b2-4930-4ff0-a3a0-8572cc559005\": 100 }, \"countByViewingCountry\": { \"HK\": 50, \"US\": 50 } } Each record represents a 5 minutes time frame(this will become clear in later chapters concerning aggregation).","title":"Event Statistics"},{"location":"backend/analytics/intro.html","text":"Analytics Introduction This is the stack that makes extensive use of the redis instance, in order to reduce load on the DynamoDB tables, and decrease lambda runtime. The Basics At the heart of the analytics stack is the heartbeat function, its endpoint being: analytics/events/{eventId}/heartbeat This collects and logs whatever is required. Current Issues This stack has a couple hard coded parameters that should probably be extracted into env variables. They are: A redis instance endpoint A VPC with VPCEndpoints for DynamoDB, S3 and SQS 3 Subnets However, compared to other preexisting architectural issues on our platform, this should be relatively easy to remedy.","title":"Intro"},{"location":"backend/analytics/intro.html#analytics-introduction","text":"This is the stack that makes extensive use of the redis instance, in order to reduce load on the DynamoDB tables, and decrease lambda runtime.","title":"Analytics Introduction"},{"location":"backend/analytics/intro.html#the-basics","text":"At the heart of the analytics stack is the heartbeat function, its endpoint being: analytics/events/{eventId}/heartbeat This collects and logs whatever is required.","title":"The Basics"},{"location":"backend/analytics/intro.html#current-issues","text":"This stack has a couple hard coded parameters that should probably be extracted into env variables. They are: A redis instance endpoint A VPC with VPCEndpoints for DynamoDB, S3 and SQS 3 Subnets However, compared to other preexisting architectural issues on our platform, this should be relatively easy to remedy.","title":"Current Issues"},{"location":"backend/analytics/issues.html","text":"Issues In the analytics stack, currently, there are a couple issues worth keeping in mind. Redis does not scale First of all, it makes heavy use of a Redis instance, and Elasticache instances does not auto scale( or at least it was not setup), so this might one day become a bottleneck. Although bottlenecks for current lambda execs and DynamoDB partition should be reached far sooner, therefore should the day come when Redis can't handle the traffic, it should be the least of your worries. Redis Memory Size There is currently no cache flushing mechanism setup for Redis. Again, his shouldn't be a problem in the foreseeable future. This should be even less of a problem once our redis instance creation is moved inside the CDK, and are scoped to each stage. Automatic(or manual) cache flush should be implemented, although this is not a current priority. Report and analytics makes use of cache records older than 5 minutes Ideally, caches older than a certain threshold should be rendered useless, and is safe to purge. How long that threshold should be is entirely dependent on application. In our case, this is true for most records. However, some never expires, and metrics like event overview, max concurrent viewer and total attended viewers relies on records throughout an event's total lifetime(to different extents). This makes cache purging difficult. Not impossible, but just hard.","title":"Issues"},{"location":"backend/analytics/issues.html#issues","text":"In the analytics stack, currently, there are a couple issues worth keeping in mind.","title":"Issues"},{"location":"backend/analytics/issues.html#redis-does-not-scale","text":"First of all, it makes heavy use of a Redis instance, and Elasticache instances does not auto scale( or at least it was not setup), so this might one day become a bottleneck. Although bottlenecks for current lambda execs and DynamoDB partition should be reached far sooner, therefore should the day come when Redis can't handle the traffic, it should be the least of your worries.","title":"Redis does not scale"},{"location":"backend/analytics/issues.html#redis-memory-size","text":"There is currently no cache flushing mechanism setup for Redis. Again, his shouldn't be a problem in the foreseeable future. This should be even less of a problem once our redis instance creation is moved inside the CDK, and are scoped to each stage. Automatic(or manual) cache flush should be implemented, although this is not a current priority.","title":"Redis Memory Size"},{"location":"backend/analytics/issues.html#report-and-analytics-makes-use-of-cache-records-older-than-5-minutes","text":"Ideally, caches older than a certain threshold should be rendered useless, and is safe to purge. How long that threshold should be is entirely dependent on application. In our case, this is true for most records. However, some never expires, and metrics like event overview, max concurrent viewer and total attended viewers relies on records throughout an event's total lifetime(to different extents). This makes cache purging difficult. Not impossible, but just hard.","title":"Report and analytics makes use of cache records older than 5 minutes"},{"location":"backend/analytics/metrics.html","text":"Metrics The following is a list of available metrics obtainable from this stack: To understand what kinda metric they provide, it is recommended to go into the functions one by one, and read them. getSessionActiveViewerCount: path: analytics/events/{eventId}/sessions/{sessionId}/viewers/active-count method: get getEventActiveViewerCount: path: analytics/events/{eventId}/viewers/active-count method: get heartbeatReport: path: analytics/events/{eventId}/heartbeat-report method: post getViewerCount: path: analytics/events/{eventId}/viewers/count method: get getAllTimeActiveViewerCount: path: analytics/events/{eventId}/viewers/all-time-active-count method: get getLiveEventStats: path: analytics/events/{eventId}/statistics/live method: get getLiveSessionStats: path: analytics/events/{eventId}/sessions/{sessionId}/statistics/live method: get getEventStats: path: analytics/events/{eventId}/statistics method: get getSessionStats: path: analytics/events/{eventId}/sessions/{sessionId}/statistics method: get getEventOverview: path: analytics/events/{eventId}/overview method: get getEventTotalAttendedViewerCount: path: analytics/events/{eventId}/viewers/total-attended-count method: get getSessionTotalAttendedViewerCount: path: analytics/events/{eventId}/sessions/{sessionId}/viewers/total-attended-count method: get getEventMaxConcurrentViewerCount: path: analytics/events/{eventId}/viewers/max-concurrent-count method: get getSessionMaxConcurrentViewerCount: path: analytics/events/{eventId}/sessions/{sessionId}/viewers/max-concurrent-count method: get getEventTotalLoginCount: path: analytics/events/{eventId}/viewers/total-login-count method: get getSessionViewingStats: path: analytics/events/{eventId}/sessions/{sessionId}/viewers/viewing-stats method: get","title":"Metrics"},{"location":"backend/analytics/metrics.html#metrics","text":"The following is a list of available metrics obtainable from this stack: To understand what kinda metric they provide, it is recommended to go into the functions one by one, and read them. getSessionActiveViewerCount: path: analytics/events/{eventId}/sessions/{sessionId}/viewers/active-count method: get getEventActiveViewerCount: path: analytics/events/{eventId}/viewers/active-count method: get heartbeatReport: path: analytics/events/{eventId}/heartbeat-report method: post getViewerCount: path: analytics/events/{eventId}/viewers/count method: get getAllTimeActiveViewerCount: path: analytics/events/{eventId}/viewers/all-time-active-count method: get getLiveEventStats: path: analytics/events/{eventId}/statistics/live method: get getLiveSessionStats: path: analytics/events/{eventId}/sessions/{sessionId}/statistics/live method: get getEventStats: path: analytics/events/{eventId}/statistics method: get getSessionStats: path: analytics/events/{eventId}/sessions/{sessionId}/statistics method: get getEventOverview: path: analytics/events/{eventId}/overview method: get getEventTotalAttendedViewerCount: path: analytics/events/{eventId}/viewers/total-attended-count method: get getSessionTotalAttendedViewerCount: path: analytics/events/{eventId}/sessions/{sessionId}/viewers/total-attended-count method: get getEventMaxConcurrentViewerCount: path: analytics/events/{eventId}/viewers/max-concurrent-count method: get getSessionMaxConcurrentViewerCount: path: analytics/events/{eventId}/sessions/{sessionId}/viewers/max-concurrent-count method: get getEventTotalLoginCount: path: analytics/events/{eventId}/viewers/total-login-count method: get getSessionViewingStats: path: analytics/events/{eventId}/sessions/{sessionId}/viewers/viewing-stats method: get","title":"Metrics"},{"location":"backend/analytics/redis-data-usage.html","text":"Redis Data Usage This chapter explains the Redis data types utilized. Redis data types reference . Admittedly, keys are sometimes named rather haphazardly, some of these collections no longer need to exist since their functionalities have been superseded by another. Sorted Sets Non repeating collections of Strings, every item is associated with a score Since every member in a sorted set is unique, only the latest is kept, older records of the same user will get overwritten. Viewing Sessions Viewing Session Start Time Collection Key: <EVENT_ID>#<SESSION_ID(optional)>#<LOCATION>#ViewingSessionStart Member Key: UserId Score: Timestamp Viewing Session End Time Collection Key: <EVENT_ID>#<SESSION_ID(optional)>#<LOCATION>#ViewingSessionEnd Member Key: UserId Score: Timestamp Alive viewers count (Current viewing users) Since ZSets are member unique, an accurate active viewer count can be obtained simply by issuing a ZCOUNT with +inf for max and T minus alive threshold for min. User Event Heartbeats Collection Key: <SESSION_ID>#Session#Heartbeat Scoped to events Member Key: UserId Score: Timestamp User Session Heartbeats Collection Key: <EVENT_ID>#Event#Heartbeat Scoped to sessions Member Key: UserId Score: Timestamp Event Statistics These require a little more explanation. They are the statistics sets. There's one that's scoped to events and another scoped to sessions. Being a sorted set, members can only have a key field and an associated numerical score. The score is the current timestamp, while the key is an encoded string containing the user's ID, IP address they're accessing the site from together with country code, user groups they're members of. User Event Stats Collection Key: <EVENT_ID>#Event#Statistics Scoped to events Member Key: <USERID>#<IPADDRESS>#(one or more <USERGROUPID>)#<COUNTRYCODE> Score: Timestamp User Session Stats Collection Key: <SESSION_ID>#Session#Statistics Scoped to sessions Member Key: <USERID>#<IPADDRESS>#(one or more <USERGROUPID>)#<COUNTRYCODE> Score: Timestamp These are poorly designed sets, and is a primary source of technical debt from inside the analytics stack. Sets Unique members, supports add, rmv, check for existence in O(1). Attendance Event User Attendance Collection Key: <EVENT_ID>#EventAttendedUsers Scoped to events Key: UserId Session User Attendance Collection Key: <SESSION_ID>#SessionAttendedUsers Grouped by SessionId Key: UserId","title":"Redis Data Usage"},{"location":"backend/analytics/redis-data-usage.html#redis-data-usage","text":"This chapter explains the Redis data types utilized. Redis data types reference . Admittedly, keys are sometimes named rather haphazardly, some of these collections no longer need to exist since their functionalities have been superseded by another.","title":"Redis Data Usage"},{"location":"backend/analytics/redis-data-usage.html#sorted-sets","text":"Non repeating collections of Strings, every item is associated with a score Since every member in a sorted set is unique, only the latest is kept, older records of the same user will get overwritten.","title":"Sorted Sets"},{"location":"backend/analytics/redis-data-usage.html#viewing-sessions","text":"Viewing Session Start Time Collection Key: <EVENT_ID>#<SESSION_ID(optional)>#<LOCATION>#ViewingSessionStart Member Key: UserId Score: Timestamp Viewing Session End Time Collection Key: <EVENT_ID>#<SESSION_ID(optional)>#<LOCATION>#ViewingSessionEnd Member Key: UserId Score: Timestamp","title":"Viewing Sessions"},{"location":"backend/analytics/redis-data-usage.html#alive-viewers-count-current-viewing-users","text":"Since ZSets are member unique, an accurate active viewer count can be obtained simply by issuing a ZCOUNT with +inf for max and T minus alive threshold for min. User Event Heartbeats Collection Key: <SESSION_ID>#Session#Heartbeat Scoped to events Member Key: UserId Score: Timestamp User Session Heartbeats Collection Key: <EVENT_ID>#Event#Heartbeat Scoped to sessions Member Key: UserId Score: Timestamp","title":"Alive viewers count (Current viewing users)"},{"location":"backend/analytics/redis-data-usage.html#event-statistics","text":"These require a little more explanation. They are the statistics sets. There's one that's scoped to events and another scoped to sessions. Being a sorted set, members can only have a key field and an associated numerical score. The score is the current timestamp, while the key is an encoded string containing the user's ID, IP address they're accessing the site from together with country code, user groups they're members of. User Event Stats Collection Key: <EVENT_ID>#Event#Statistics Scoped to events Member Key: <USERID>#<IPADDRESS>#(one or more <USERGROUPID>)#<COUNTRYCODE> Score: Timestamp User Session Stats Collection Key: <SESSION_ID>#Session#Statistics Scoped to sessions Member Key: <USERID>#<IPADDRESS>#(one or more <USERGROUPID>)#<COUNTRYCODE> Score: Timestamp These are poorly designed sets, and is a primary source of technical debt from inside the analytics stack.","title":"Event Statistics"},{"location":"backend/analytics/redis-data-usage.html#sets","text":"Unique members, supports add, rmv, check for existence in O(1).","title":"Sets"},{"location":"backend/analytics/redis-data-usage.html#attendance","text":"Event User Attendance Collection Key: <EVENT_ID>#EventAttendedUsers Scoped to events Key: UserId Session User Attendance Collection Key: <SESSION_ID>#SessionAttendedUsers Grouped by SessionId Key: UserId","title":"Attendance"},{"location":"backend/deepdive/data-schema.html","text":"Data Schema Placeholder Under construction.","title":"Data Schema"},{"location":"backend/deepdive/data-schema.html#data-schema","text":"","title":"Data Schema"},{"location":"backend/deepdive/data-schema.html#placeholder","text":"Under construction.","title":"Placeholder"},{"location":"backend/deepdive/database-structure.html","text":"Nova's Database Structure Single Table Design Nova's database structure is problematic in its current state. Below is our most common DynamoDB setup(in cloudformation syntax, simplified): Properties: KeySchema: - AttributeName: pk KeyType: HASH - AttributeName: sk KeyType: RANGE GlobalSecondaryIndexes: - IndexName: gsi0-index KeySchema: - AttributeName: sk KeyType: HASH - AttributeName: pk KeyType: RANGE - IndexName: gsi1-index KeySchema: - AttributeName: sk KeyType: HASH - AttributeName: gsi1sk KeyType: RANGE - IndexName: gsi2-index KeySchema: - AttributeName: gsi2pk KeyType: HASH - AttributeName: gsi2sk KeyType: RANGE - IndexName: gsi3-index KeySchema: - AttributeName: gsi3pk KeyType: HASH - AttributeName: gsi3sk KeyType: RANGE The first thing you may notice, is that attribute names are generic. They only reflect the DB's key schema. This was an abuse of the Single Table Design . The Single Table Design, simply put, is just placing everything in the same table, and attributes can have variable meaning according to the object type. It's a great way to boost performance and save cost, at the cost of developer agility. It's an abuse in our case mainly because it was done for the wrong reasons. The intention of picking this route I was told, was to minimize efforts in table design. The rationale was that, having a very generic table with multiple generic indices eliminates any future need to modify/update the table. New data objects can be invented on the fly together with new access patterns. This is of course, very misguided. DynamoDB design goes hand in hand with knowing your access patterns beforehand. Using a single table design without knowledge of access patterns introduces a cognitive overhead, but brings none of the intended benefits. Example Data Consider this example of an actual event profile, plucked from the development server, with most non-key attributes stripped: { \"pk\": \"0d48c37b-0b35-40cf-8c36-e23ddc97440f\", \"eventId\": \"0d48c37b-0b35-40cf-8c36-e23ddc97440f\", \"sk\": \"Event#Profile\", \"gsi1sk\": \"355dada4-fb99-4bd9-801f-bea92cd03a1d\", \"channelId\": \"355dada4-fb99-4bd9-801f-bea92cd03a1d\" } You might have noticed that both eventId and channelId are duplicated into pk and gsi1sk respectively. In Nova 's context, Events are scoped under Channels . To retrieve this object, issue a get operation with the following parameters: // Remember HASH key + SORT key combinations are globally unique? pk: 0d48c37b-0b35-40cf-8c36-e23ddc97440f sk: Event#Profile To retrieve a list of event objects under the same channel however, a query should be issued: Index Name: gsi1-index sk: Event#Profile (Remember sk is the HASH key for gsi-index?) gsi1sk: \"=\" 355dada4-fb99-4bd9-801f-bea92cd03a1d There's a lot of cognitive overhead just to remember the index name, the HASH key for that index, and that channelId was overloaded into gsi1sk . This is why one should almost never query the database directly with a single table design. This is also why we have wrapper classes( StorageBase ) that encapsulate our data schema and access patterns.","title":"Nova's Database Structure"},{"location":"backend/deepdive/database-structure.html#novas-database-structure","text":"","title":"Nova's Database Structure"},{"location":"backend/deepdive/database-structure.html#single-table-design","text":"Nova's database structure is problematic in its current state. Below is our most common DynamoDB setup(in cloudformation syntax, simplified): Properties: KeySchema: - AttributeName: pk KeyType: HASH - AttributeName: sk KeyType: RANGE GlobalSecondaryIndexes: - IndexName: gsi0-index KeySchema: - AttributeName: sk KeyType: HASH - AttributeName: pk KeyType: RANGE - IndexName: gsi1-index KeySchema: - AttributeName: sk KeyType: HASH - AttributeName: gsi1sk KeyType: RANGE - IndexName: gsi2-index KeySchema: - AttributeName: gsi2pk KeyType: HASH - AttributeName: gsi2sk KeyType: RANGE - IndexName: gsi3-index KeySchema: - AttributeName: gsi3pk KeyType: HASH - AttributeName: gsi3sk KeyType: RANGE The first thing you may notice, is that attribute names are generic. They only reflect the DB's key schema. This was an abuse of the Single Table Design . The Single Table Design, simply put, is just placing everything in the same table, and attributes can have variable meaning according to the object type. It's a great way to boost performance and save cost, at the cost of developer agility. It's an abuse in our case mainly because it was done for the wrong reasons. The intention of picking this route I was told, was to minimize efforts in table design. The rationale was that, having a very generic table with multiple generic indices eliminates any future need to modify/update the table. New data objects can be invented on the fly together with new access patterns. This is of course, very misguided. DynamoDB design goes hand in hand with knowing your access patterns beforehand. Using a single table design without knowledge of access patterns introduces a cognitive overhead, but brings none of the intended benefits.","title":"Single Table Design"},{"location":"backend/deepdive/database-structure.html#example-data","text":"Consider this example of an actual event profile, plucked from the development server, with most non-key attributes stripped: { \"pk\": \"0d48c37b-0b35-40cf-8c36-e23ddc97440f\", \"eventId\": \"0d48c37b-0b35-40cf-8c36-e23ddc97440f\", \"sk\": \"Event#Profile\", \"gsi1sk\": \"355dada4-fb99-4bd9-801f-bea92cd03a1d\", \"channelId\": \"355dada4-fb99-4bd9-801f-bea92cd03a1d\" } You might have noticed that both eventId and channelId are duplicated into pk and gsi1sk respectively. In Nova 's context, Events are scoped under Channels . To retrieve this object, issue a get operation with the following parameters: // Remember HASH key + SORT key combinations are globally unique? pk: 0d48c37b-0b35-40cf-8c36-e23ddc97440f sk: Event#Profile To retrieve a list of event objects under the same channel however, a query should be issued: Index Name: gsi1-index sk: Event#Profile (Remember sk is the HASH key for gsi-index?) gsi1sk: \"=\" 355dada4-fb99-4bd9-801f-bea92cd03a1d There's a lot of cognitive overhead just to remember the index name, the HASH key for that index, and that channelId was overloaded into gsi1sk . This is why one should almost never query the database directly with a single table design. This is also why we have wrapper classes( StorageBase ) that encapsulate our data schema and access patterns.","title":"Example Data"},{"location":"backend/deepdive/dynamo-basics.html","text":"DynamoDB Basics Key Structure Each item in a DynamoDB table is uniquely identified by a primary key(the HASH key, or the PARTITION key) + secondary key(sometimes called the RANGE key or the SORT key) combination. The attributes designated to be HASH and SORT keys have to be pre-declared during table creation. Note: A DynamoDB can be created without a range key, in which case the hash key will be the sole identifier, and can only appear once per table. Let's take for instance a table with userId as HASH key and recordKey as SORT . With the following item: { \"userId\": \"user-001\", \"recordKey\": \"Profile\" // ...omitted } Since HASH + SORT key combinations are unique, an item with the above keys can only appear once in the entire table. Any put operation with those specific keys will overwrite whatever item that was there beforehand. Partitions A partition in DynamoDB simply means a collection of items that share the same HASH key. These two are items in the same partition: [ { \"userId\": \"user-001\", \"recordKey\": \"Profile\" }, { \"userId\": \"user-001\", \"recordKey\": \"Birthday\" } ] While these two are not: [ { \"userId\": \"user-001\", \"recordKey\": \"Profile\" }, { \"userId\": \"user-002\", \"recordKey\": \"Profile\" } ] Queries DynamoDB allows querying by HASH key and/or a condition on the SORT key. Consider a table with the following items: [ { \"userId\": \"user-001\", \"recordKey\": \"Profile\" }, { \"userId\": \"user-001\", \"recordKey\": \"Friend#user-002\" }, { \"userId\": \"user-001\", \"recordKey\": \"Friend#user-003\" }, { \"userId\": \"user-001\", \"recordKey\": \"Session#2022-10-07\" }, { \"userId\": \"user-001\", \"recordKey\": \"Session#2022-10-15\" }, { \"userId\": \"user-001\", \"recordKey\": \"Session#2022-10-23\" } ] You can issue queries in following format: (Pseudo query, not actual DynamoDB syntax) HASH partition userId equals user-001 SORT condition recordKey equals Profile You'd get one result, for obvious reasons. You could also issue a query with no condition on SORT key, in which case you'd receive all 5 items above. Or you could give it a begins with condition. HASH partition userId equals user-001 SORT condition recordKey begins with Friend# You'd get the following items: [ { \"userId\": \"user-001\", \"recordKey\": \"Friend#user-002\" }, { \"userId\": \"user-001\", \"recordKey\": \"Friend#user-003\" } ] And also: HASH partition userId equals user-001 SORT condition recordKey greater than Session#2022-10-14 In which case you'd get: [ { \"userId\": \"user-001\", \"recordKey\": \"Session#2022-10-15\" }, { \"userId\": \"user-001\", \"recordKey\": \"Session#2022-10-23\" } ] Caveat: Since recordKey is a string attribute, ASCII sort is employed, meaning\" Session#2022-10-10\" would be sorted before \"Session#2022-10-2\". Secondary Indices Secondary indices in DynamoDB are copies of the data grouped and sorted by attributes different to main table. Let's look at this example: [ { \"userId\": \"user-001\", \"recordKey\": \"Profile\", \"timestamp\": 12345 }, { \"userId\": \"user-002\", \"recordKey\": \"Profile\", \"timestamp\": 23456 }, { \"userId\": \"user-003\", \"recordKey\": \"Profile\", \"timestamp\": 34567 } ] You'd like to query for a list of profile objects, but you want only the ones created after a certain time. Normally you can't, since you can only query by HASH key, in this case, userId . With secondary indices you can. All that is required is a secondary index created for the table with recordKey as HASH and timestamp as SORT . After which you may perform queries on the index just as you would on the main table. One thing of notes is that, HASH SORT combinations on a secondary index are not unique, meaning there may exist more than 1 records with the same recordKey plus timestamp . This is also why you cannot perform single get operations on indices. References This is another excellent guide on DynamoDB.","title":"DynamoDB Basics"},{"location":"backend/deepdive/dynamo-basics.html#dynamodb-basics","text":"","title":"DynamoDB Basics"},{"location":"backend/deepdive/dynamo-basics.html#key-structure","text":"Each item in a DynamoDB table is uniquely identified by a primary key(the HASH key, or the PARTITION key) + secondary key(sometimes called the RANGE key or the SORT key) combination. The attributes designated to be HASH and SORT keys have to be pre-declared during table creation. Note: A DynamoDB can be created without a range key, in which case the hash key will be the sole identifier, and can only appear once per table. Let's take for instance a table with userId as HASH key and recordKey as SORT . With the following item: { \"userId\": \"user-001\", \"recordKey\": \"Profile\" // ...omitted } Since HASH + SORT key combinations are unique, an item with the above keys can only appear once in the entire table. Any put operation with those specific keys will overwrite whatever item that was there beforehand.","title":"Key Structure"},{"location":"backend/deepdive/dynamo-basics.html#partitions","text":"A partition in DynamoDB simply means a collection of items that share the same HASH key. These two are items in the same partition: [ { \"userId\": \"user-001\", \"recordKey\": \"Profile\" }, { \"userId\": \"user-001\", \"recordKey\": \"Birthday\" } ] While these two are not: [ { \"userId\": \"user-001\", \"recordKey\": \"Profile\" }, { \"userId\": \"user-002\", \"recordKey\": \"Profile\" } ]","title":"Partitions"},{"location":"backend/deepdive/dynamo-basics.html#queries","text":"DynamoDB allows querying by HASH key and/or a condition on the SORT key. Consider a table with the following items: [ { \"userId\": \"user-001\", \"recordKey\": \"Profile\" }, { \"userId\": \"user-001\", \"recordKey\": \"Friend#user-002\" }, { \"userId\": \"user-001\", \"recordKey\": \"Friend#user-003\" }, { \"userId\": \"user-001\", \"recordKey\": \"Session#2022-10-07\" }, { \"userId\": \"user-001\", \"recordKey\": \"Session#2022-10-15\" }, { \"userId\": \"user-001\", \"recordKey\": \"Session#2022-10-23\" } ] You can issue queries in following format: (Pseudo query, not actual DynamoDB syntax) HASH partition userId equals user-001 SORT condition recordKey equals Profile You'd get one result, for obvious reasons. You could also issue a query with no condition on SORT key, in which case you'd receive all 5 items above. Or you could give it a begins with condition. HASH partition userId equals user-001 SORT condition recordKey begins with Friend# You'd get the following items: [ { \"userId\": \"user-001\", \"recordKey\": \"Friend#user-002\" }, { \"userId\": \"user-001\", \"recordKey\": \"Friend#user-003\" } ] And also: HASH partition userId equals user-001 SORT condition recordKey greater than Session#2022-10-14 In which case you'd get: [ { \"userId\": \"user-001\", \"recordKey\": \"Session#2022-10-15\" }, { \"userId\": \"user-001\", \"recordKey\": \"Session#2022-10-23\" } ] Caveat: Since recordKey is a string attribute, ASCII sort is employed, meaning\" Session#2022-10-10\" would be sorted before \"Session#2022-10-2\".","title":"Queries"},{"location":"backend/deepdive/dynamo-basics.html#secondary-indices","text":"Secondary indices in DynamoDB are copies of the data grouped and sorted by attributes different to main table. Let's look at this example: [ { \"userId\": \"user-001\", \"recordKey\": \"Profile\", \"timestamp\": 12345 }, { \"userId\": \"user-002\", \"recordKey\": \"Profile\", \"timestamp\": 23456 }, { \"userId\": \"user-003\", \"recordKey\": \"Profile\", \"timestamp\": 34567 } ] You'd like to query for a list of profile objects, but you want only the ones created after a certain time. Normally you can't, since you can only query by HASH key, in this case, userId . With secondary indices you can. All that is required is a secondary index created for the table with recordKey as HASH and timestamp as SORT . After which you may perform queries on the index just as you would on the main table. One thing of notes is that, HASH SORT combinations on a secondary index are not unique, meaning there may exist more than 1 records with the same recordKey plus timestamp . This is also why you cannot perform single get operations on indices.","title":"Secondary Indices"},{"location":"backend/deepdive/dynamo-basics.html#references","text":"This is another excellent guide on DynamoDB.","title":"References"},{"location":"backend/deepdive/storage-classes.html","text":"Storage Classes Placeholder","title":"Storage Classes"},{"location":"backend/deepdive/storage-classes.html#storage-classes","text":"","title":"Storage Classes"},{"location":"backend/deepdive/storage-classes.html#placeholder","text":"","title":"Placeholder"},{"location":"backend/deployment/deployment.html","text":"Lambda/API Deployment Get Ready Before we begin, ensure that you've obtained a copy of the ENV variables required for all API stacks. They are named as such: .env.<STAGE_NAME>.json .secrets.<STAGE_NAME>.json And they look somewhat like this: Deployment Order The API stacks can be deployed in any order except for 2 of them ! And they are: nwv2-api-admin-iam and nwv2-api-viewer-auth They need to be deployed first, in no particular order. The reason for that is they contain declarations to 2 authorizer functions that are imported and used in almost all other stacks. Relevant excerpts from nwv2-api-admin-iam 's serverless.yml : Deploy We'll use nwv2-api-admin-iam as an example. Change directory into the project root, run: npm install After which an npm postinstall script will be automatically triggered, the scripts simply goes into a submodule's folder and run install from within, and then run a TypeScript transpiler. Make sure both env files are placed under project root. (The nwv2-api-admin-iam stack actually only requires one, but it doesn't hurt.) Now for the actual deployment, run: sls deploy --stage <STAGE_NAME> # for example: sls deploy --stage dev If deploying with a different AWSCLI profile: sls deploy --stage <STAGE_NAME> --profile <PROFILE_NAME> # for example: sls deploy --stage dev --profile someguy Or even to a different region: sls deploy --stage <STAGE_NAME> --profile <PROFILE_NAME> --region <REGION_CODE> # for example: sls deploy --stage dev --profile someguy --region ap-southeast-1 If any of these variables are not provided, the following defaults will be used instead: stage: dev profile: nwv2Admin This is why we recommend naming your credential nwv2Admin . region: ap-southeast-1 Relevant excerpt: Example output from a successful deployment: . . . functions: sharedAdminAuthorizer: novaweb-api-admin-iam-dev-sharedAdminAuthorizer createUser: novaweb-api-admin-iam-dev-createUser deleteUser: novaweb-api-admin-iam-dev-deleteUser getUsers: novaweb-api-admin-iam-dev-getUsers updateUser: novaweb-api-admin-iam-dev-updateUser authenticate: novaweb-api-admin-iam-dev-authenticate triggerPasswordReset: novaweb-api-admin-iam-dev-triggerPasswordReset passwordReset: novaweb-api-admin-iam-dev-passwordReset logout: novaweb-api-admin-iam-dev-logout layers: None Serverless: Removing old service artifacts from S3... Rinse and repeat. Important ( nwv2-api-analytics ) Although you should be able to deploy all 7 API stacks successfully just by doing the above, there's one caveat. Our analytics stack nwv2-api-analytics contains references to AWS resources created neither by the CDK nor the Serverless Stacks. The resources being referred to are: A VPC with: A security group, and A couple attached subnets A Redis database created inside the above VPC VPC Endpoints for the following resources: DynamoDB (Gateway endpoint) S3 (Gateway endpoint) SQS (Interface endpoint) Relevant excerpt: The hard coded IDs reference existing resources, which, should not cause a problem if you're deploying to the same AWS account. They are by design shared between stacks anyway. In any case this should be refactored out. Either by extracting into an env file, or move resource declaration into the CDK project.","title":"Deployment"},{"location":"backend/deployment/deployment.html#lambdaapi-deployment","text":"","title":"Lambda/API Deployment"},{"location":"backend/deployment/deployment.html#get-ready","text":"Before we begin, ensure that you've obtained a copy of the ENV variables required for all API stacks. They are named as such: .env.<STAGE_NAME>.json .secrets.<STAGE_NAME>.json And they look somewhat like this:","title":"Get Ready"},{"location":"backend/deployment/deployment.html#deployment-order","text":"The API stacks can be deployed in any order except for 2 of them ! And they are: nwv2-api-admin-iam and nwv2-api-viewer-auth They need to be deployed first, in no particular order. The reason for that is they contain declarations to 2 authorizer functions that are imported and used in almost all other stacks. Relevant excerpts from nwv2-api-admin-iam 's serverless.yml :","title":"Deployment Order"},{"location":"backend/deployment/deployment.html#deploy","text":"We'll use nwv2-api-admin-iam as an example. Change directory into the project root, run: npm install After which an npm postinstall script will be automatically triggered, the scripts simply goes into a submodule's folder and run install from within, and then run a TypeScript transpiler. Make sure both env files are placed under project root. (The nwv2-api-admin-iam stack actually only requires one, but it doesn't hurt.) Now for the actual deployment, run: sls deploy --stage <STAGE_NAME> # for example: sls deploy --stage dev If deploying with a different AWSCLI profile: sls deploy --stage <STAGE_NAME> --profile <PROFILE_NAME> # for example: sls deploy --stage dev --profile someguy Or even to a different region: sls deploy --stage <STAGE_NAME> --profile <PROFILE_NAME> --region <REGION_CODE> # for example: sls deploy --stage dev --profile someguy --region ap-southeast-1 If any of these variables are not provided, the following defaults will be used instead: stage: dev profile: nwv2Admin This is why we recommend naming your credential nwv2Admin . region: ap-southeast-1 Relevant excerpt: Example output from a successful deployment: . . . functions: sharedAdminAuthorizer: novaweb-api-admin-iam-dev-sharedAdminAuthorizer createUser: novaweb-api-admin-iam-dev-createUser deleteUser: novaweb-api-admin-iam-dev-deleteUser getUsers: novaweb-api-admin-iam-dev-getUsers updateUser: novaweb-api-admin-iam-dev-updateUser authenticate: novaweb-api-admin-iam-dev-authenticate triggerPasswordReset: novaweb-api-admin-iam-dev-triggerPasswordReset passwordReset: novaweb-api-admin-iam-dev-passwordReset logout: novaweb-api-admin-iam-dev-logout layers: None Serverless: Removing old service artifacts from S3... Rinse and repeat.","title":"Deploy"},{"location":"backend/deployment/deployment.html#important-nwv2-api-analytics","text":"Although you should be able to deploy all 7 API stacks successfully just by doing the above, there's one caveat. Our analytics stack nwv2-api-analytics contains references to AWS resources created neither by the CDK nor the Serverless Stacks. The resources being referred to are: A VPC with: A security group, and A couple attached subnets A Redis database created inside the above VPC VPC Endpoints for the following resources: DynamoDB (Gateway endpoint) S3 (Gateway endpoint) SQS (Interface endpoint) Relevant excerpt: The hard coded IDs reference existing resources, which, should not cause a problem if you're deploying to the same AWS account. They are by design shared between stacks anyway. In any case this should be refactored out. Either by extracting into an env file, or move resource declaration into the CDK project.","title":"Important (nwv2-api-analytics)"},{"location":"backend/deployment/faqs.html","text":"Frequently Asked Questions I pulled the code, but the nwv2-api-lib folder is empty! That's because it is a submodule. Git doc entry on submodules . Simply put, they are git's way of having nested repositories. A submodule is not exactly a standalone repo, although it behaves like one. To initialize and fetch a submodule, run the following in the parent repo's root folder: git submodule update --init --recursive It will initialize the submodule and reset it to the correct commit. Why is npm install looping forever? In almost all API stacks, there is a npm postinstall declared inside package.json . postinstall is an npm lifecycle that can be hooked into, it executes after every npm install command. In our particular projects, the postinstall script cd s into a submodule folder and runs npm install there. Now if a package.json file cannot be found, npm will search upwards recursively until one is found, usually it'll find the one under project root, hence the loop.","title":"FAQs"},{"location":"backend/deployment/faqs.html#frequently-asked-questions","text":"","title":"Frequently Asked Questions"},{"location":"backend/deployment/faqs.html#i-pulled-the-code-but-the-nwv2-api-lib-folder-is-empty","text":"That's because it is a submodule. Git doc entry on submodules . Simply put, they are git's way of having nested repositories. A submodule is not exactly a standalone repo, although it behaves like one. To initialize and fetch a submodule, run the following in the parent repo's root folder: git submodule update --init --recursive It will initialize the submodule and reset it to the correct commit.","title":"I pulled the code, but the nwv2-api-lib folder is empty!"},{"location":"backend/deployment/faqs.html#why-is-npm-install-looping-forever","text":"In almost all API stacks, there is a npm postinstall declared inside package.json . postinstall is an npm lifecycle that can be hooked into, it executes after every npm install command. In our particular projects, the postinstall script cd s into a submodule folder and runs npm install there. Now if a package.json file cannot be found, npm will search upwards recursively until one is found, usually it'll find the one under project root, hence the loop.","title":"Why is npm install looping forever?"},{"location":"backend/deployment/post-deploy.html","text":"Post Deploy Setup Why? After a successful deployment, there are 2 more things you'll need to do before the platform can operate. And they are: Initialize the database with a root user. Initialize the database with one set of html template for each AuthPreset You probably have no idea what this means at this stage, but that's ok. For now, just know that it has to be done. Pull the project Both of these things can be done with the nwv2-platform-init project, so go pull it now. There's a submodule in it too, so please remember to initialize it. Double check that src/nwv2-api-lib is not empty, and then we can move on to the next step. Env variables The nwv2-platform-init uses dotenv . So the required env file is a key value pair instead of a json , and you'll need to create one yourself. First create a file and name it .env . Paste this into the file: Stage= AWS_PROFILE=nwv2Admin AWS_REGION=ap-southeast-1 BucketName= UserImportBucket= CloudfrontDomain= ViewerTable= ViewerSubmissionTable= AdminTable= LogTable= ViewerHostname= EmailFrom= NovaBotToken= ApiHostname= RedisEndpoint= MailchimpKey= MailchimpServerPrefix= MandrillKey= You don't need all of the values for this to work, just these: Stage= AWS_PROFILE=nwv2Admin AWS_REGION=ap-southeast-1 AdminTable= ViewerHostname= Stage would be the name of your stage. AdminTable would be the name of the corresponding table, in most cases it should be called something like this: novaweb-<STAGE>-AdminTable ViewerHostname is the full URL for the event viewer site. You can obtain all of those values from the regular .env.<STAGE>.json file. Actual Setup Run: # Make sure the submodule is pulled and inited! npm install After which the postinstall script should run. Make sure the necessary env values are filled in. Run: npm run-script setup-root-admin Ensure it ran without errors. And then run: npm run-script init-default-templates Again, ensure there are no errors. That's it!","title":"Post Deploy Setup"},{"location":"backend/deployment/post-deploy.html#post-deploy-setup","text":"","title":"Post Deploy Setup"},{"location":"backend/deployment/post-deploy.html#why","text":"After a successful deployment, there are 2 more things you'll need to do before the platform can operate. And they are: Initialize the database with a root user. Initialize the database with one set of html template for each AuthPreset You probably have no idea what this means at this stage, but that's ok. For now, just know that it has to be done.","title":"Why?"},{"location":"backend/deployment/post-deploy.html#pull-the-project","text":"Both of these things can be done with the nwv2-platform-init project, so go pull it now. There's a submodule in it too, so please remember to initialize it. Double check that src/nwv2-api-lib is not empty, and then we can move on to the next step.","title":"Pull the project"},{"location":"backend/deployment/post-deploy.html#env-variables","text":"The nwv2-platform-init uses dotenv . So the required env file is a key value pair instead of a json , and you'll need to create one yourself. First create a file and name it .env . Paste this into the file: Stage= AWS_PROFILE=nwv2Admin AWS_REGION=ap-southeast-1 BucketName= UserImportBucket= CloudfrontDomain= ViewerTable= ViewerSubmissionTable= AdminTable= LogTable= ViewerHostname= EmailFrom= NovaBotToken= ApiHostname= RedisEndpoint= MailchimpKey= MailchimpServerPrefix= MandrillKey= You don't need all of the values for this to work, just these: Stage= AWS_PROFILE=nwv2Admin AWS_REGION=ap-southeast-1 AdminTable= ViewerHostname= Stage would be the name of your stage. AdminTable would be the name of the corresponding table, in most cases it should be called something like this: novaweb-<STAGE>-AdminTable ViewerHostname is the full URL for the event viewer site. You can obtain all of those values from the regular .env.<STAGE>.json file.","title":"Env variables"},{"location":"backend/deployment/post-deploy.html#actual-setup","text":"Run: # Make sure the submodule is pulled and inited! npm install After which the postinstall script should run. Make sure the necessary env values are filled in. Run: npm run-script setup-root-admin Ensure it ran without errors. And then run: npm run-script init-default-templates Again, ensure there are no errors. That's it!","title":"Actual Setup"},{"location":"backend/deployment/prerequisites.html","text":"Prerequisites AWS CLI Please make sure you have the following ready: A valid set of credentials that has the necessary permissions. A profile named nwv2Admin (Optional, but recommended. This is the default profile our SLS stacks use for deployment.) Try running this command after setup: aws sts get-caller-identity --profile nwv2Admin You should get a response similar to this one: Node Not much can be said about this one. Most newer versions should work. Verified versions: - v15.5.1 (CICD Server is running this) - v12.22.1 Serverless Framework Official Getting Started page . In any case, it is recommended to install with NPM: npm install -g serverless Run this to verify that it works: sls --version sls is simply a shortcut for the serverless command.","title":"Prerequisites"},{"location":"backend/deployment/prerequisites.html#prerequisites","text":"","title":"Prerequisites"},{"location":"backend/deployment/prerequisites.html#aws-cli","text":"Please make sure you have the following ready: A valid set of credentials that has the necessary permissions. A profile named nwv2Admin (Optional, but recommended. This is the default profile our SLS stacks use for deployment.) Try running this command after setup: aws sts get-caller-identity --profile nwv2Admin You should get a response similar to this one:","title":"AWS CLI"},{"location":"backend/deployment/prerequisites.html#node","text":"Not much can be said about this one. Most newer versions should work. Verified versions: - v15.5.1 (CICD Server is running this) - v12.22.1","title":"Node"},{"location":"backend/deployment/prerequisites.html#serverless-framework","text":"Official Getting Started page . In any case, it is recommended to install with NPM: npm install -g serverless Run this to verify that it works: sls --version sls is simply a shortcut for the serverless command.","title":"Serverless Framework"},{"location":"environments/deployment.html","text":"Deployment Procedures(Environments) dev Deployments to the dev stage(and all other personal stages) is governed mainly by common sense. Sometimes you'd deploy local changes, sometimes a full stack deploy, sometimes only one lambda function is deployed. Just be a good sport and talk to your colleagues before doing something that might influence their workflow. If you plan to break things and cleanse DB often, try to do it in your own environment. Each night, our CICD pipeline will redeploy and overwrite the dev stage with code on all master branches. This exists to catch compile time errors early and reset any personal changes that are not pushed and exist only on your local repositories. After a nightly deployment, a jmeter integration test script will be run against the newly deployed stage, in order to catch runtime errors and mistakes early. More info on this in a later chapter. sandbox Do not deploy local and makeshift changes to this stage. All changes should be deployed from the sandbox branch, having been preliminarily tested by developers. Changes should be merged to sandbox from master , and should be fast forward merges only. Ex: git merge master --ff-only . prod Deployment should only be made to prod after the current sandbox version is passed by the QA team and deemed ready for production, and a right time is found for deployment. Do not deploying local and makeshift changes to this stage. Again, deployments are made from the prod branch, which should be merged from sandbox , fast forward only. Occasionally, bugs will be found, some will be critical(completely breaks user experience). When that happens, discuss with your leads and determine whether or not this is a bug that warrants immediate action. The goal should be to avoid making makeshift hotfixes, see if it can wait for the next deployment. In that case that it can't wait, fixes should be made and committed and deployed to prod directly. This will result in prod having commits that are not on dev nor sandbox , and will look something like this: This means the next merge from sandbox to prod cannot be a fast forward. In cases like this, avoid merging backwards from prod to dev , as that dirties the commit history and creates confusion. Cherry-pick the change to dev , and in the next production deployment cycle, reset the prod branch, remove the offending commit, and once again merge from sandbox , fast forward only. Afterwards, perform a force push to reset the commit history. Cherry-pick the hotfix: Reset prod : Fast forward and force push:","title":"Deployment Procedures(Environments)"},{"location":"environments/deployment.html#deployment-proceduresenvironments","text":"","title":"Deployment Procedures(Environments)"},{"location":"environments/deployment.html#dev","text":"Deployments to the dev stage(and all other personal stages) is governed mainly by common sense. Sometimes you'd deploy local changes, sometimes a full stack deploy, sometimes only one lambda function is deployed. Just be a good sport and talk to your colleagues before doing something that might influence their workflow. If you plan to break things and cleanse DB often, try to do it in your own environment. Each night, our CICD pipeline will redeploy and overwrite the dev stage with code on all master branches. This exists to catch compile time errors early and reset any personal changes that are not pushed and exist only on your local repositories. After a nightly deployment, a jmeter integration test script will be run against the newly deployed stage, in order to catch runtime errors and mistakes early. More info on this in a later chapter.","title":"dev"},{"location":"environments/deployment.html#sandbox","text":"Do not deploy local and makeshift changes to this stage. All changes should be deployed from the sandbox branch, having been preliminarily tested by developers. Changes should be merged to sandbox from master , and should be fast forward merges only. Ex: git merge master --ff-only .","title":"sandbox"},{"location":"environments/deployment.html#prod","text":"Deployment should only be made to prod after the current sandbox version is passed by the QA team and deemed ready for production, and a right time is found for deployment. Do not deploying local and makeshift changes to this stage. Again, deployments are made from the prod branch, which should be merged from sandbox , fast forward only. Occasionally, bugs will be found, some will be critical(completely breaks user experience). When that happens, discuss with your leads and determine whether or not this is a bug that warrants immediate action. The goal should be to avoid making makeshift hotfixes, see if it can wait for the next deployment. In that case that it can't wait, fixes should be made and committed and deployed to prod directly. This will result in prod having commits that are not on dev nor sandbox , and will look something like this: This means the next merge from sandbox to prod cannot be a fast forward. In cases like this, avoid merging backwards from prod to dev , as that dirties the commit history and creates confusion. Cherry-pick the change to dev , and in the next production deployment cycle, reset the prod branch, remove the offending commit, and once again merge from sandbox , fast forward only. Afterwards, perform a force push to reset the commit history. Cherry-pick the hotfix: Reset prod : Fast forward and force push:","title":"prod"},{"location":"environments/intro.html","text":"Deployment Environments How many? There are a total of 3 core stages, and they are: dev sandbox prod Where? They are all located on Nova 's core account. This is not ideal, and was a cheap and quick way to avoid heavy administrative overhead. AWS recommends having separate accounts for staging and production. In fact, latest AWS best practice guides recommends having separate accounts PER PROJECT. This is certainly going to introduce admin overhead, and a much longer setup period. It would also require automation tooling to properly manage. With the right CICD setup, it can be done, and the safety it provides is probably worth it. The dev Stage(and any other personal stages) These are for development purposes, database cleanse would sometimes happen unannounced(especially if it's your own personal stage). There are automation in place for nightly deploys. Points to the latest master branch of every project. The sandbox Stage This is what is usually called the QA environment. It's for QA team's test drive of newly implemented features. Team leads should be consulted before database cleanses take place. Points to the latest sandbox branch. The prod Stage The production environment. Do not deploy unannounced. Do not do anything unannounced. Talk with project managers to determine the best time for production deployment(preferably dates with no impending events in the coming few days). All hands on deck.","title":"Introduction"},{"location":"environments/intro.html#deployment-environments","text":"","title":"Deployment Environments"},{"location":"environments/intro.html#how-many","text":"There are a total of 3 core stages, and they are: dev sandbox prod","title":"How many?"},{"location":"environments/intro.html#where","text":"They are all located on Nova 's core account. This is not ideal, and was a cheap and quick way to avoid heavy administrative overhead. AWS recommends having separate accounts for staging and production. In fact, latest AWS best practice guides recommends having separate accounts PER PROJECT. This is certainly going to introduce admin overhead, and a much longer setup period. It would also require automation tooling to properly manage. With the right CICD setup, it can be done, and the safety it provides is probably worth it.","title":"Where?"},{"location":"environments/intro.html#the-dev-stageand-any-other-personal-stages","text":"These are for development purposes, database cleanse would sometimes happen unannounced(especially if it's your own personal stage). There are automation in place for nightly deploys. Points to the latest master branch of every project.","title":"The dev Stage(and any other personal stages)"},{"location":"environments/intro.html#the-sandbox-stage","text":"This is what is usually called the QA environment. It's for QA team's test drive of newly implemented features. Team leads should be consulted before database cleanses take place. Points to the latest sandbox branch.","title":"The sandbox Stage"},{"location":"environments/intro.html#the-prod-stage","text":"The production environment. Do not deploy unannounced. Do not do anything unannounced. Talk with project managers to determine the best time for production deployment(preferably dates with no impending events in the coming few days). All hands on deck.","title":"The prod Stage"},{"location":"environments/qol.html","text":"Deployment QOL Automation Since we have 7 api projects and 4 site project(so far). Merging and deploying 11 projects everytime a deployment is made, not to mention having to properly remember and manage different env secrets, giving the proper cli stage variables, is not only tiresome, it's also prone to human errors. Therefore, for deployment, we have a Jenkins CICD server at our disposal. The details of our Jenkins server will be in another chapter. Our Jenkins home page should look like this(after login): Depending on your level of access, what you see may differ from this. Navigate into a stage's folder, we'll take sandbox for instance: And then press the play button on the project you wish to deploy: It will copy the necessary env files over, together with api secrets for third party apis, install required modules and deploy for you. The script it runs looks a bit like this: npm install cp ~/nova-env/.env.<STAGE>.json $WORKSPACE/ cp ~/nova-env/.secrets.<STAGE>.json $WORKSPACE/ sls deploy --stage <STAGE> --profile default Jobs can run concurrently, meaning you can safely press play on all 11 projects, and patiently let it do its job. Deployments are also done from their respective branch, therefore sandbox and prod deployments should to an extent be idempotent. Recommended shell Scripts to Have Even with the help of Jenkins, managing 11 git repositories is still somewhat a cognitively demanding operation. Therefore, it is recommended to have a number of local bash scripts that can help with your everyday tasks. Copies env files from server Our envs are stored on the jenkins instance, if you have ssh access, you can conveniently copy them to your local environment with scp example(all examples are in batch(windows)): echo off scp <server_domain>:~/nova-env/.env.*.json ./ scp <server_domain>:~/nova-env/.secrets.*.json ./ for /D %%i in (\"*\") do ( xcopy .env.*.json %~dp0\\%%i\\ /Y xcopy .secrets.*.json %~dp0\\%%i\\ /Y ) pause Merge current master to sandbox This will be done about once a week, so it's immensely helpful if you don't need to merge and push all 11 projects manually Example: (in batch) echo off for /D %%i in (\"*\") do ( cd %~dp0%%i call git switch master --recurse-submodules --discard-changes call git pull --ff-only call git submodule update --init --recursive call git switch sandbox --recurse-submodules --discard-changes call git merge master --ff-only call git push call git switch master --recurse-submodules --discard-changes ) pause Merge current sandbox to prod This one goes without saying Update all submodules to the latest This is not done very frequently, usually submodule\\ versions are managed per projects, since every project is usually on a different submodule version, but on the rare occasions that you made substantial updates to the submodule and is updating submodules for all api projects frequently(a couple times a day), this is immensely helpful.","title":"Deployment QOL"},{"location":"environments/qol.html#deployment-qol","text":"","title":"Deployment QOL"},{"location":"environments/qol.html#automation","text":"Since we have 7 api projects and 4 site project(so far). Merging and deploying 11 projects everytime a deployment is made, not to mention having to properly remember and manage different env secrets, giving the proper cli stage variables, is not only tiresome, it's also prone to human errors. Therefore, for deployment, we have a Jenkins CICD server at our disposal. The details of our Jenkins server will be in another chapter. Our Jenkins home page should look like this(after login): Depending on your level of access, what you see may differ from this. Navigate into a stage's folder, we'll take sandbox for instance: And then press the play button on the project you wish to deploy: It will copy the necessary env files over, together with api secrets for third party apis, install required modules and deploy for you. The script it runs looks a bit like this: npm install cp ~/nova-env/.env.<STAGE>.json $WORKSPACE/ cp ~/nova-env/.secrets.<STAGE>.json $WORKSPACE/ sls deploy --stage <STAGE> --profile default Jobs can run concurrently, meaning you can safely press play on all 11 projects, and patiently let it do its job. Deployments are also done from their respective branch, therefore sandbox and prod deployments should to an extent be idempotent.","title":"Automation"},{"location":"environments/qol.html#recommended-shell-scripts-to-have","text":"Even with the help of Jenkins, managing 11 git repositories is still somewhat a cognitively demanding operation. Therefore, it is recommended to have a number of local bash scripts that can help with your everyday tasks. Copies env files from server Our envs are stored on the jenkins instance, if you have ssh access, you can conveniently copy them to your local environment with scp example(all examples are in batch(windows)): echo off scp <server_domain>:~/nova-env/.env.*.json ./ scp <server_domain>:~/nova-env/.secrets.*.json ./ for /D %%i in (\"*\") do ( xcopy .env.*.json %~dp0\\%%i\\ /Y xcopy .secrets.*.json %~dp0\\%%i\\ /Y ) pause Merge current master to sandbox This will be done about once a week, so it's immensely helpful if you don't need to merge and push all 11 projects manually Example: (in batch) echo off for /D %%i in (\"*\") do ( cd %~dp0%%i call git switch master --recurse-submodules --discard-changes call git pull --ff-only call git submodule update --init --recursive call git switch sandbox --recurse-submodules --discard-changes call git merge master --ff-only call git push call git switch master --recurse-submodules --discard-changes ) pause Merge current sandbox to prod This one goes without saying Update all submodules to the latest This is not done very frequently, usually submodule\\ versions are managed per projects, since every project is usually on a different submodule version, but on the rare occasions that you made substantial updates to the submodule and is updating submodules for all api projects frequently(a couple times a day), this is immensely helpful.","title":"Recommended shell Scripts to Have"},{"location":"frontend/admin-panel.html","text":"Admin Panel (a.k.a. Console)","title":"Admin Panel"},{"location":"frontend/admin-panel.html#admin-panel-aka-console","text":"","title":"Admin Panel (a.k.a. Console)"},{"location":"frontend/event-viewer.html","text":"Event Viewer","title":"Event Viewer"},{"location":"frontend/event-viewer.html#event-viewer","text":"","title":"Event Viewer"},{"location":"frontend/extension-poll.html","text":"Poll Extension","title":"Poll"},{"location":"frontend/extension-poll.html#poll-extension","text":"","title":"Poll Extension"},{"location":"frontend/extension-survey.html","text":"Survey Extension","title":"Survey"},{"location":"frontend/extension-survey.html#survey-extension","text":"","title":"Survey Extension"},{"location":"frontend/extensions.html","text":"Extensions","title":"Extensions"},{"location":"frontend/extensions.html#extensions","text":"","title":"Extensions"},{"location":"frontend/prerequisites.html","text":"Prerequisites NodeJS Official site Bootstrap-Vue Documentation Bootstrap 4.5 Documentation Vue.js 2.x Documentation Vue CLI Installation Vue.js devtools Official site Installation Chrome Extension Firefox Add-ons Theme Documentation Live Demo AWS CLI AWS Command Line Interface","title":"Prerequisites"},{"location":"frontend/prerequisites.html#prerequisites","text":"","title":"Prerequisites"},{"location":"frontend/prerequisites.html#nodejs","text":"Official site","title":"NodeJS"},{"location":"frontend/prerequisites.html#bootstrap-vue","text":"Documentation","title":"Bootstrap-Vue"},{"location":"frontend/prerequisites.html#bootstrap-45","text":"Documentation","title":"Bootstrap 4.5"},{"location":"frontend/prerequisites.html#vuejs-2x","text":"Documentation","title":"Vue.js 2.x"},{"location":"frontend/prerequisites.html#vue-cli","text":"Installation","title":"Vue CLI"},{"location":"frontend/prerequisites.html#vuejs-devtools","text":"Official site Installation Chrome Extension Firefox Add-ons","title":"Vue.js devtools"},{"location":"frontend/prerequisites.html#theme","text":"Documentation Live Demo","title":"Theme"},{"location":"frontend/prerequisites.html#aws-cli","text":"AWS Command Line Interface","title":"AWS CLI"},{"location":"intro/domains.html","text":"Domains All of Novaweb's online platforms are placed under this domain: Official Project novaweb.live There are a total of six sites: Production Environment (latest stable) admin-panel: console.novaweb.live event-viewer: events.novaweb.live Sandbox Environment admin-panel: sandbox-console.novaweb.live event-viewer: sandbox-events.novaweb.live Dev Environment admin-panel: dev-console.novaweb.live event-viewer: dev-events.novaweb.live Other Client Projects hkritaevent.hk (v2.0) admin-panel: console.hkritaevent.hk event-viewer: hkritaevent.hk immuno-oncologyhk2021.com (v1.5) admin-panel: console.immuno-oncologyhk2021.com event-viewer: events.immuno-oncologyhk2021.com","title":"Domains"},{"location":"intro/domains.html#domains","text":"All of Novaweb's online platforms are placed under this domain:","title":"Domains"},{"location":"intro/domains.html#official-project","text":"","title":"Official Project"},{"location":"intro/domains.html#novaweblive","text":"There are a total of six sites: Production Environment (latest stable) admin-panel: console.novaweb.live event-viewer: events.novaweb.live Sandbox Environment admin-panel: sandbox-console.novaweb.live event-viewer: sandbox-events.novaweb.live Dev Environment admin-panel: dev-console.novaweb.live event-viewer: dev-events.novaweb.live","title":"novaweb.live"},{"location":"intro/domains.html#other-client-projects","text":"","title":"Other Client Projects"},{"location":"intro/domains.html#hkritaeventhk-v20","text":"admin-panel: console.hkritaevent.hk event-viewer: hkritaevent.hk","title":"hkritaevent.hk (v2.0)"},{"location":"intro/domains.html#immuno-oncologyhk2021com-v15","text":"admin-panel: console.immuno-oncologyhk2021.com event-viewer: events.immuno-oncologyhk2021.com","title":"immuno-oncologyhk2021.com (v1.5)"},{"location":"intro/repo-migration.html","text":"Migrating Repositories Migrating repos should be relatively painless, with one very important caveat. Submodules. If you are familiar with them. You can safely skip this article. What is a git submodule? A git submodule is declared inside a file named .gitmodules in the root folder. This file is committed and is tracked by git. The insides looks like this: [submodule \"functions/lib/nwv2-api-lib\"] path = functions/lib/nwv2-api-lib url = ../nwv2-api-lib.git The url param can take a relative path or a full path, some of the projects uses an absolute path, some relative. An absolute path would look like this: [submodule \"functions/lib/nwv2-api-lib\"] path = functions/lib/nwv2-api-lib url = git@gitlab.com:yellowvls/project/nova/nwv2-api-lib.git You can probably imagine, if the library module is migrated, it will stop working, since the path would become incorrect. It also probably wouldn't work if you use http instead of git protocol for fetching. So in most cases, a relative path is recommended, unless the submodule is hosted on a different server. As for relative paths, if the library module is not placed under a path with the same relativity, it won't work either. So during migration, care should be taken to ensure submodules remain in the same location relative to their parent. And that absolute submodule paths are updated to reflect the migration.","title":"Migrating Repositories"},{"location":"intro/repo-migration.html#migrating-repositories","text":"Migrating repos should be relatively painless, with one very important caveat. Submodules. If you are familiar with them. You can safely skip this article. What is a git submodule? A git submodule is declared inside a file named .gitmodules in the root folder. This file is committed and is tracked by git. The insides looks like this: [submodule \"functions/lib/nwv2-api-lib\"] path = functions/lib/nwv2-api-lib url = ../nwv2-api-lib.git The url param can take a relative path or a full path, some of the projects uses an absolute path, some relative. An absolute path would look like this: [submodule \"functions/lib/nwv2-api-lib\"] path = functions/lib/nwv2-api-lib url = git@gitlab.com:yellowvls/project/nova/nwv2-api-lib.git You can probably imagine, if the library module is migrated, it will stop working, since the path would become incorrect. It also probably wouldn't work if you use http instead of git protocol for fetching. So in most cases, a relative path is recommended, unless the submodule is hosted on a different server. As for relative paths, if the library module is not placed under a path with the same relativity, it won't work either. So during migration, care should be taken to ensure submodules remain in the same location relative to their parent. And that absolute submodule paths are updated to reflect the migration.","title":"Migrating Repositories"},{"location":"intro/repos.html","text":"Repositories Backend API API Gateway stacks managed with Serverless Framework nwv2-api-admin-iam Serverless stack Admin authentications Contains authorizer shared by other admin stacks MUST BE DEPLOYED FIRST! https://gitlab.com/nova35/novaweb/v2/api/nwv2-api-admin nwv2-api-admin Core admin APIs Creating and modifying channels , events , sessions , and other platform resources. https://gitlab.com/nova35/novaweb/v2/api/nwv2-api-admin nwv2-api-analytics What the name suggests This project contains references to AWS resources not deployed by the CDK See Deployment section for details. https://gitlab.com/nova35/novaweb/v2/api/nwv2-api-analytics nwv2-api-message-queue Asynchronous batch processing that involves lambda fan-outs with SQS https://gitlab.com/nova35/novaweb/v2/api/nwv2-api-msg-queue nwv2-api-event-interaction APIs pertaining to interactive elements of an event: Polls, Q&As, Surveys, anything that involves end users submitting things. https://gitlab.com/nova35/novaweb/v2/api/nwv2-api-event-interaction nwv2-api-viewer-auth Viewer authentications Contains authorizer shared by other viewer stacks MUST BE DEPLOYED FIRST! Either before or after nwv2-api-admin-iam https://gitlab.com/nova35/novaweb/v2/api/nwv2-api-viewer-auth nwv2-api-viewer-events The server side rendering part of Novaweb platform Extremely convoluted, very hard to read even with intimate knowledge of business logic. Consult Louis. https://gitlab.com/nova35/novaweb/v2/api/nwv2-api-viewer-event nwv2-api-lib Submodule shared by the other API repos https://gitlab.com/nova35/novaweb/v2/api/nwv2-api-lib Front-end Clients All client projects are using VueJS Framework and BootstrapVue . nwv2-client-admin-panel Administrative Panel (a.k.a Console) and Mission Control VueJS, Vue-Bootstrap https://gitlab.com/nova35/novaweb/v2/sites/nwv2-client-admin-panel nwv2-client-event-viewer Event Viewer VueJS https://gitlab.com/nova35/novaweb/v2/sites/nwv2-client-event-viewer nwv2-client-extension-poll Poll Client VueJS, Vue-Bootstrap https://gitlab.com/nova35/novaweb/v2/sites/nwv2-client-extension-poll nwv2-client-extension-survey Survey Client VueJS, Vue-Bootstrap https://gitlab.com/nova35/novaweb/v2/sites/nwv2-client-extension-survey nwv2-client-lib Submodule shared by the other Client repos https://gitlab.com/nova35/novaweb/v2/sites/nwv2-client-lib Utility Repos nwv2cdk The installer of novaweb It contains main script to launch the novaweb main stack, its fundamental infrastructure and resources on AWS, uses AWS Cloud Development Kit (CDK) https://gitlab.com/nova35/novaweb/v2/nwv2-cdk nwv2-dev-tools Connect to AWS resources from your local computer With the exception of some EC2 resources like Redis, which can only be connected to from VPC Suitable for quick tests and debugging without having to deploy code to a Lambda Function https://gitlab.com/nova35/novaweb/v2/utilities/nwv2-dev-tools novaweb-jmeter Automated API tests that ensures platform functionalities are up and running. High but not 100% coverage, especially newer features. Also contains scripts for stress testing purposes. https://gitlab.com/nova35/novaweb/v2/utilities/nwv2-jmeter-integration-tests nwv2-platform-init Contains scripts that initializes a newly deployed platform, mainly: Initializes the console with a root admin user. Copies default html templates into the database. https://gitlab.com/nova35/novaweb/v2/utilities/nwv2-platform-init developer-manual Source files of this documentation site Uses Markdown and MkDocs https://gitlab.com/nova35/novaweb/v2/developer-manual","title":"Repositories"},{"location":"intro/repos.html#repositories","text":"","title":"Repositories"},{"location":"intro/repos.html#backend-api","text":"API Gateway stacks managed with Serverless Framework nwv2-api-admin-iam Serverless stack Admin authentications Contains authorizer shared by other admin stacks MUST BE DEPLOYED FIRST! https://gitlab.com/nova35/novaweb/v2/api/nwv2-api-admin nwv2-api-admin Core admin APIs Creating and modifying channels , events , sessions , and other platform resources. https://gitlab.com/nova35/novaweb/v2/api/nwv2-api-admin nwv2-api-analytics What the name suggests This project contains references to AWS resources not deployed by the CDK See Deployment section for details. https://gitlab.com/nova35/novaweb/v2/api/nwv2-api-analytics nwv2-api-message-queue Asynchronous batch processing that involves lambda fan-outs with SQS https://gitlab.com/nova35/novaweb/v2/api/nwv2-api-msg-queue nwv2-api-event-interaction APIs pertaining to interactive elements of an event: Polls, Q&As, Surveys, anything that involves end users submitting things. https://gitlab.com/nova35/novaweb/v2/api/nwv2-api-event-interaction nwv2-api-viewer-auth Viewer authentications Contains authorizer shared by other viewer stacks MUST BE DEPLOYED FIRST! Either before or after nwv2-api-admin-iam https://gitlab.com/nova35/novaweb/v2/api/nwv2-api-viewer-auth nwv2-api-viewer-events The server side rendering part of Novaweb platform Extremely convoluted, very hard to read even with intimate knowledge of business logic. Consult Louis. https://gitlab.com/nova35/novaweb/v2/api/nwv2-api-viewer-event nwv2-api-lib Submodule shared by the other API repos https://gitlab.com/nova35/novaweb/v2/api/nwv2-api-lib","title":"Backend API"},{"location":"intro/repos.html#front-end-clients","text":"All client projects are using VueJS Framework and BootstrapVue . nwv2-client-admin-panel Administrative Panel (a.k.a Console) and Mission Control VueJS, Vue-Bootstrap https://gitlab.com/nova35/novaweb/v2/sites/nwv2-client-admin-panel nwv2-client-event-viewer Event Viewer VueJS https://gitlab.com/nova35/novaweb/v2/sites/nwv2-client-event-viewer nwv2-client-extension-poll Poll Client VueJS, Vue-Bootstrap https://gitlab.com/nova35/novaweb/v2/sites/nwv2-client-extension-poll nwv2-client-extension-survey Survey Client VueJS, Vue-Bootstrap https://gitlab.com/nova35/novaweb/v2/sites/nwv2-client-extension-survey nwv2-client-lib Submodule shared by the other Client repos https://gitlab.com/nova35/novaweb/v2/sites/nwv2-client-lib","title":"Front-end Clients"},{"location":"intro/repos.html#utility-repos","text":"nwv2cdk The installer of novaweb It contains main script to launch the novaweb main stack, its fundamental infrastructure and resources on AWS, uses AWS Cloud Development Kit (CDK) https://gitlab.com/nova35/novaweb/v2/nwv2-cdk nwv2-dev-tools Connect to AWS resources from your local computer With the exception of some EC2 resources like Redis, which can only be connected to from VPC Suitable for quick tests and debugging without having to deploy code to a Lambda Function https://gitlab.com/nova35/novaweb/v2/utilities/nwv2-dev-tools novaweb-jmeter Automated API tests that ensures platform functionalities are up and running. High but not 100% coverage, especially newer features. Also contains scripts for stress testing purposes. https://gitlab.com/nova35/novaweb/v2/utilities/nwv2-jmeter-integration-tests nwv2-platform-init Contains scripts that initializes a newly deployed platform, mainly: Initializes the console with a root admin user. Copies default html templates into the database. https://gitlab.com/nova35/novaweb/v2/utilities/nwv2-platform-init developer-manual Source files of this documentation site Uses Markdown and MkDocs https://gitlab.com/nova35/novaweb/v2/developer-manual","title":"Utility Repos"},{"location":"jenkins/intro.html","text":"Introduction By now, you should know that we have a Jenkins server at our disposal. Our Jenkins server is hosted on AWS, in a EC2 instance. Resources In Use EC2 Instance(t3.xlarge) Elastic IP Network Load Balancer Elastic Block Storage(100G) Structure The instance is an Amazon Linux instance, based on RHEL, if you've used red hats before, this should be similar. Jenkins is installed as a yum package. Jenkins itself does not handle SSL, so an Nginx reverse proxy is set up to handle SSL encryption. And then it's TLS terminated by AWS, using a Network Load Balancer.","title":"Introduction"},{"location":"jenkins/intro.html#introduction","text":"By now, you should know that we have a Jenkins server at our disposal. Our Jenkins server is hosted on AWS, in a EC2 instance.","title":"Introduction"},{"location":"jenkins/intro.html#resources-in-use","text":"EC2 Instance(t3.xlarge) Elastic IP Network Load Balancer Elastic Block Storage(100G)","title":"Resources In Use"},{"location":"jenkins/intro.html#structure","text":"The instance is an Amazon Linux instance, based on RHEL, if you've used red hats before, this should be similar. Jenkins is installed as a yum package. Jenkins itself does not handle SSL, so an Nginx reverse proxy is set up to handle SSL encryption. And then it's TLS terminated by AWS, using a Network Load Balancer.","title":"Structure"},{"location":"jenkins/management.html","text":"Managing Jenkins Creation and management of jobs is out of the scope of this guide, so that is something you'll have to explore on your own. This guide aims to cover certain configurations that may be vital to Jenkins being operational, and where to find them. If you have been granted administrative access, this button should be available to you: This is where all management options are grouped under. Important Barebones Jenkins is very configurable, but it does not provide a lot of QOL features. It is very easy to mess up, accidentally delete jobs/users, provide wrong permission to the wrong people, or simply just render the server unusable. So beware. Updating git Credentials Git credentials(rsa ssh keys) can be found under: Manage Jenkins > Manage Credentials Jenkins Doc After which the key will be available for selection during jobs configuration: Access Control Manage Jenkins > Configure Global Security Under Authorization , make sure Project-based Matrix Authorization Strategy is checked. You can hover over individual permissions to see a description of them. This can be configured per jobs/folder as well. IMPORTANT!! Be careful not to remove administrative permission from yourself/everyone if it would result in no one having admin rights. This is irreversible and Jenkins would be left without admins. The only way to remedy this is to SSH into the instance, and reset Jenkins backing data store manually. Jenkins CLI Jenkins provides a CLI for easier management when it comes to batch creating/updating jobs. It can be done with jenkins-cli.jar , over SSH , HTTP , or even Websockets . This makes jobs duplication a lot safer. Jenkins Doc There are also libraries that wraps the CLI calls for you, like this one . Jenkins jobs are saved in XML , meaning you can export them and perform your choice of version control if you so wish. New Users There are 2 ways for adding new users: Manage Jenkins > Manage Users > Create User Provide username and password to users Users can then set a new password by: Clicking on their avatar icon on the top right Click Configure Manage Jenkins > Configure Global Security Check Allow users to sign up under Security Realm Resetting a user's password Manage Jenkins > Manage Users > Select username > Configure","title":"Managing Jenkins"},{"location":"jenkins/management.html#managing-jenkins","text":"Creation and management of jobs is out of the scope of this guide, so that is something you'll have to explore on your own. This guide aims to cover certain configurations that may be vital to Jenkins being operational, and where to find them. If you have been granted administrative access, this button should be available to you: This is where all management options are grouped under.","title":"Managing Jenkins"},{"location":"jenkins/management.html#important","text":"Barebones Jenkins is very configurable, but it does not provide a lot of QOL features. It is very easy to mess up, accidentally delete jobs/users, provide wrong permission to the wrong people, or simply just render the server unusable. So beware.","title":"Important"},{"location":"jenkins/management.html#updating-git-credentials","text":"Git credentials(rsa ssh keys) can be found under: Manage Jenkins > Manage Credentials Jenkins Doc After which the key will be available for selection during jobs configuration:","title":"Updating git Credentials"},{"location":"jenkins/management.html#access-control","text":"Manage Jenkins > Configure Global Security Under Authorization , make sure Project-based Matrix Authorization Strategy is checked. You can hover over individual permissions to see a description of them. This can be configured per jobs/folder as well. IMPORTANT!! Be careful not to remove administrative permission from yourself/everyone if it would result in no one having admin rights. This is irreversible and Jenkins would be left without admins. The only way to remedy this is to SSH into the instance, and reset Jenkins backing data store manually.","title":"Access Control"},{"location":"jenkins/management.html#jenkins-cli","text":"Jenkins provides a CLI for easier management when it comes to batch creating/updating jobs. It can be done with jenkins-cli.jar , over SSH , HTTP , or even Websockets . This makes jobs duplication a lot safer. Jenkins Doc There are also libraries that wraps the CLI calls for you, like this one . Jenkins jobs are saved in XML , meaning you can export them and perform your choice of version control if you so wish.","title":"Jenkins CLI"},{"location":"jenkins/management.html#new-users","text":"There are 2 ways for adding new users: Manage Jenkins > Manage Users > Create User Provide username and password to users Users can then set a new password by: Clicking on their avatar icon on the top right Click Configure Manage Jenkins > Configure Global Security Check Allow users to sign up under Security Realm","title":"New Users"},{"location":"jenkins/management.html#resetting-a-users-password","text":"Manage Jenkins > Manage Users > Select username > Configure","title":"Resetting a user's password"}]}